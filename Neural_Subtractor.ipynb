{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 減法器\n",
    "# 本程式為建構一深度學習模型，將其訓練成一個減法器，輸入一段三位數 - 三位數之算式，計算及答案。 並比較不同神經元個數、Epoch次數之影響。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 匯入所需library，主要使用keras建立神經網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Concatenate\n",
    "from keras.models import Model\n",
    "import keras.backend as bk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自行生成減法算式資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Finish!\n"
     ]
    }
   ],
   "source": [
    "DIGITS = 3\n",
    "MAXLEN = DIGITS + 1 + DIGITS\n",
    "chars = '0123456789- '\n",
    "\n",
    "# Generate Data\n",
    "questions = []\n",
    "expected = []\n",
    "\n",
    "print('Generating data...')\n",
    "\n",
    "for a in range(0, 1000):\n",
    "    for b in range(0, 1000):\n",
    "        if a > b:    # 若a大於b，才產生減法運算資料\n",
    "            q = '{}-{}'.format(str(a), str(b))    # 將 -號 插進兩數字中，產生數學式的字串\n",
    "            query = q + ' ' * (MAXLEN - len(q))\n",
    "            ans = str(a - b)\n",
    "            ans += ' ' * (DIGITS + 1 - len(ans))\n",
    "            questions.append(query)\n",
    "            expected.append(ans)\n",
    "        \n",
    "print('Finish!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    def __init__(self, chars):\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))      # char to integer\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))      # integer to char\n",
    "    \n",
    "    def encode(self, C, num_rows):\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return \"\".join(self.indices_char[i] for i in x)\n",
    "      \n",
    "ctable = CharacterTable(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將產生好的資料化為 One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "Finish!\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.int)  \n",
    "y = np.zeros((len(expected), DIGITS + 1, len(chars)), dtype=np.int)\n",
    "\n",
    "# One-Hot encoding\n",
    "for i, sentence in enumerate(questions):\n",
    "    x[i] = ctable.encode(sentence, MAXLEN)\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
    "    \n",
    "print('Finish!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拆分訓練、驗證、測試資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "(319680, 7, 12)\n",
      "(319680, 4, 12)\n",
      "Validation Data:\n",
      "(79920, 7, 12)\n",
      "(79920, 4, 12)\n",
      "Testing Data:\n",
      "(99900, 7, 12)\n",
      "(99900, 4, 12)\n"
     ]
    }
   ],
   "source": [
    "# 拆分訓練、驗證、測試資料集\n",
    "test_ratio = 0.2\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=test_ratio, random_state=0)\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=test_ratio, random_state=0)\n",
    "\n",
    "print('Training Data:')\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "\n",
    "print('Validation Data:')\n",
    "print(valid_x.shape)\n",
    "print(valid_y.shape)\n",
    "\n",
    "print('Testing Data:')\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將資料攤平\n",
    "ctrain_x = train_x.reshape(train_x.shape[0], -1, 1).squeeze(axis=2)\n",
    "ctest_x = test_x.reshape(test_x.shape[0], -1, 1).squeeze(axis=2)\n",
    "ctrain_y = train_y.reshape(train_y.shape[0], -1, 1).squeeze(axis=2)\n",
    "ctest_y = test_y.reshape(test_y.shape[0], -1, 1).squeeze(axis=2)\n",
    "cvalid_x = valid_x.reshape(valid_x.shape[0], -1, 1).squeeze(axis=2)\n",
    "cvalid_y = valid_y.reshape(valid_y.shape[0], -1, 1).squeeze(axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 固定訓練10個 epochs, 以不同的隱藏層神經元個數訓練，比較 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/10\n",
      "319680/319680 [==============================] - 17s 52us/step - loss: 10.4693 - acc: 0.0213 - val_loss: 9.4347 - val_acc: 0.0410\n",
      "Epoch 2/10\n",
      "319680/319680 [==============================] - 16s 50us/step - loss: 9.1718 - acc: 0.0730 - val_loss: 9.0103 - val_acc: 0.0991\n",
      "Epoch 3/10\n",
      "319680/319680 [==============================] - 16s 50us/step - loss: 8.9119 - acc: 0.1014 - val_loss: 8.8510 - val_acc: 0.1389\n",
      "Epoch 4/10\n",
      "319680/319680 [==============================] - 16s 50us/step - loss: 8.7935 - acc: 0.1205 - val_loss: 8.7599 - val_acc: 0.1337\n",
      "Epoch 5/10\n",
      "319680/319680 [==============================] - 16s 49us/step - loss: 8.7290 - acc: 0.1361 - val_loss: 8.7072 - val_acc: 0.1468\n",
      "Epoch 6/10\n",
      "319680/319680 [==============================] - 17s 53us/step - loss: 8.6834 - acc: 0.1478 - val_loss: 8.6696 - val_acc: 0.1417\n",
      "Epoch 7/10\n",
      "319680/319680 [==============================] - 16s 49us/step - loss: 8.6473 - acc: 0.1565 - val_loss: 8.6386 - val_acc: 0.1386\n",
      "Epoch 8/10\n",
      "319680/319680 [==============================] - 15s 48us/step - loss: 8.6161 - acc: 0.1645 - val_loss: 8.6123 - val_acc: 0.1449\n",
      "Epoch 9/10\n",
      "319680/319680 [==============================] - 15s 48us/step - loss: 8.5943 - acc: 0.1709 - val_loss: 8.5919 - val_acc: 0.1689\n",
      "Epoch 10/10\n",
      "319680/319680 [==============================] - 16s 50us/step - loss: 8.5761 - acc: 0.1763 - val_loss: 8.5758 - val_acc: 0.2040\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/10\n",
      "319680/319680 [==============================] - 17s 52us/step - loss: 9.7529 - acc: 0.0250 - val_loss: 8.6993 - val_acc: 0.1031\n",
      "Epoch 2/10\n",
      "319680/319680 [==============================] - 17s 53us/step - loss: 8.2081 - acc: 0.1325 - val_loss: 7.9027 - val_acc: 0.1852\n",
      "Epoch 3/10\n",
      "319680/319680 [==============================] - 16s 50us/step - loss: 7.7879 - acc: 0.1758 - val_loss: 7.6819 - val_acc: 0.1646\n",
      "Epoch 4/10\n",
      "319680/319680 [==============================] - 18s 55us/step - loss: 7.5934 - acc: 0.1931 - val_loss: 7.5269 - val_acc: 0.1991\n",
      "Epoch 5/10\n",
      "319680/319680 [==============================] - 17s 54us/step - loss: 7.4850 - acc: 0.2022 - val_loss: 7.4518 - val_acc: 0.2035\n",
      "Epoch 6/10\n",
      "319680/319680 [==============================] - 17s 53us/step - loss: 7.4158 - acc: 0.2075 - val_loss: 7.3929 - val_acc: 0.2264\n",
      "Epoch 7/10\n",
      "319680/319680 [==============================] - 16s 51us/step - loss: 7.3630 - acc: 0.2098 - val_loss: 7.3430 - val_acc: 0.2377\n",
      "Epoch 8/10\n",
      "319680/319680 [==============================] - 15s 48us/step - loss: 7.3117 - acc: 0.2130 - val_loss: 7.3022 - val_acc: 0.2759\n",
      "Epoch 9/10\n",
      "319680/319680 [==============================] - 15s 47us/step - loss: 7.2719 - acc: 0.2148 - val_loss: 7.2639 - val_acc: 0.2248\n",
      "Epoch 10/10\n",
      "319680/319680 [==============================] - 15s 46us/step - loss: 7.2420 - acc: 0.2171 - val_loss: 7.2445 - val_acc: 0.2368\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/10\n",
      "319680/319680 [==============================] - 17s 52us/step - loss: 9.1013 - acc: 0.0647 - val_loss: 7.7930 - val_acc: 0.1597\n",
      "Epoch 2/10\n",
      "319680/319680 [==============================] - 17s 54us/step - loss: 7.5950 - acc: 0.1665 - val_loss: 7.4535 - val_acc: 0.1777\n",
      "Epoch 3/10\n",
      "319680/319680 [==============================] - 20s 63us/step - loss: 7.3548 - acc: 0.1976 - val_loss: 7.2833 - val_acc: 0.1783\n",
      "Epoch 4/10\n",
      "319680/319680 [==============================] - 19s 60us/step - loss: 7.2024 - acc: 0.2115 - val_loss: 7.1312 - val_acc: 0.2139\n",
      "Epoch 5/10\n",
      "319680/319680 [==============================] - 18s 56us/step - loss: 7.0792 - acc: 0.2191 - val_loss: 7.0345 - val_acc: 0.2260\n",
      "Epoch 6/10\n",
      "319680/319680 [==============================] - 20s 62us/step - loss: 6.9961 - acc: 0.2242 - val_loss: 6.9589 - val_acc: 0.2625\n",
      "Epoch 7/10\n",
      "319680/319680 [==============================] - 19s 58us/step - loss: 6.9326 - acc: 0.2272 - val_loss: 6.9108 - val_acc: 0.1947\n",
      "Epoch 8/10\n",
      "319680/319680 [==============================] - 18s 55us/step - loss: 6.8820 - acc: 0.2301 - val_loss: 6.8592 - val_acc: 0.2412\n",
      "Epoch 9/10\n",
      "319680/319680 [==============================] - 17s 55us/step - loss: 6.8429 - acc: 0.2327 - val_loss: 6.8195 - val_acc: 0.2289\n",
      "Epoch 10/10\n",
      "319680/319680 [==============================] - 18s 55us/step - loss: 6.8127 - acc: 0.2336 - val_loss: 6.8161 - val_acc: 0.2035\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/10\n",
      "319680/319680 [==============================] - 19s 59us/step - loss: 8.7881 - acc: 0.0717 - val_loss: 7.5960 - val_acc: 0.1539\n",
      "Epoch 2/10\n",
      "319680/319680 [==============================] - 19s 59us/step - loss: 7.3927 - acc: 0.1847 - val_loss: 7.2256 - val_acc: 0.2180\n",
      "Epoch 3/10\n",
      "319680/319680 [==============================] - 18s 56us/step - loss: 7.1091 - acc: 0.2120 - val_loss: 7.0307 - val_acc: 0.2219\n",
      "Epoch 4/10\n",
      "319680/319680 [==============================] - 18s 56us/step - loss: 6.9545 - acc: 0.2222 - val_loss: 6.9076 - val_acc: 0.2102\n",
      "Epoch 5/10\n",
      "319680/319680 [==============================] - 19s 58us/step - loss: 6.8566 - acc: 0.2271 - val_loss: 6.8202 - val_acc: 0.2624\n",
      "Epoch 6/10\n",
      "319680/319680 [==============================] - 18s 55us/step - loss: 6.7886 - acc: 0.2306 - val_loss: 6.7647 - val_acc: 0.2987\n",
      "Epoch 7/10\n",
      "319680/319680 [==============================] - 19s 58us/step - loss: 6.7371 - acc: 0.2315 - val_loss: 6.7240 - val_acc: 0.2202\n",
      "Epoch 8/10\n",
      "319680/319680 [==============================] - 18s 56us/step - loss: 6.6943 - acc: 0.2339 - val_loss: 6.6793 - val_acc: 0.2573\n",
      "Epoch 9/10\n",
      "319680/319680 [==============================] - 18s 55us/step - loss: 6.6596 - acc: 0.2360 - val_loss: 6.6484 - val_acc: 0.2151\n",
      "Epoch 10/10\n",
      "319680/319680 [==============================] - 19s 58us/step - loss: 6.6265 - acc: 0.2364 - val_loss: 6.6178 - val_acc: 0.2552\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/10\n",
      "319680/319680 [==============================] - 20s 61us/step - loss: 8.6071 - acc: 0.0908 - val_loss: 7.4775 - val_acc: 0.1515\n",
      "Epoch 2/10\n",
      "319680/319680 [==============================] - 20s 61us/step - loss: 7.2473 - acc: 0.1961 - val_loss: 7.0773 - val_acc: 0.1831\n",
      "Epoch 3/10\n",
      "319680/319680 [==============================] - 21s 65us/step - loss: 6.9657 - acc: 0.2188 - val_loss: 6.8733 - val_acc: 0.2622\n",
      "Epoch 4/10\n",
      "319680/319680 [==============================] - 19s 61us/step - loss: 6.8182 - acc: 0.2288 - val_loss: 6.7688 - val_acc: 0.2303\n",
      "Epoch 5/10\n",
      "319680/319680 [==============================] - 20s 64us/step - loss: 6.7233 - acc: 0.2321 - val_loss: 6.6814 - val_acc: 0.2557\n",
      "Epoch 6/10\n",
      "319680/319680 [==============================] - 20s 63us/step - loss: 6.6538 - acc: 0.2354 - val_loss: 6.6260 - val_acc: 0.2112\n",
      "Epoch 7/10\n",
      "319680/319680 [==============================] - 20s 64us/step - loss: 6.5987 - acc: 0.2371 - val_loss: 6.5744 - val_acc: 0.2269\n",
      "Epoch 8/10\n",
      "319680/319680 [==============================] - 21s 66us/step - loss: 6.5494 - acc: 0.2385 - val_loss: 6.5358 - val_acc: 0.2034\n",
      "Epoch 9/10\n",
      "319680/319680 [==============================] - 20s 62us/step - loss: 6.5082 - acc: 0.2402 - val_loss: 6.4899 - val_acc: 0.2938\n",
      "Epoch 10/10\n",
      "319680/319680 [==============================] - 19s 61us/step - loss: 6.4714 - acc: 0.2418 - val_loss: 6.4570 - val_acc: 0.2734\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/10\n",
      "319680/319680 [==============================] - 21s 66us/step - loss: 8.3878 - acc: 0.1144 - val_loss: 7.2938 - val_acc: 0.1993\n",
      "Epoch 2/10\n",
      "319680/319680 [==============================] - 21s 66us/step - loss: 7.0770 - acc: 0.2132 - val_loss: 6.9132 - val_acc: 0.2141\n",
      "Epoch 3/10\n",
      "319680/319680 [==============================] - 20s 62us/step - loss: 6.8051 - acc: 0.2277 - val_loss: 6.7189 - val_acc: 0.2538\n",
      "Epoch 4/10\n",
      "319680/319680 [==============================] - 21s 65us/step - loss: 6.6551 - acc: 0.2332 - val_loss: 6.6144 - val_acc: 0.2415\n",
      "Epoch 5/10\n",
      "319680/319680 [==============================] - 22s 69us/step - loss: 6.5615 - acc: 0.2362 - val_loss: 6.5409 - val_acc: 0.2141\n",
      "Epoch 6/10\n",
      "319680/319680 [==============================] - 22s 68us/step - loss: 6.4919 - acc: 0.2386 - val_loss: 6.4699 - val_acc: 0.2488\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319680/319680 [==============================] - 22s 69us/step - loss: 6.4372 - acc: 0.2396 - val_loss: 6.4248 - val_acc: 0.2494\n",
      "Epoch 8/10\n",
      "319680/319680 [==============================] - 23s 72us/step - loss: 6.3937 - acc: 0.2409 - val_loss: 6.3963 - val_acc: 0.2250\n",
      "Epoch 9/10\n",
      "319680/319680 [==============================] - 23s 71us/step - loss: 6.3571 - acc: 0.2416 - val_loss: 6.3624 - val_acc: 0.2627\n",
      "Epoch 10/10\n",
      "319680/319680 [==============================] - 21s 65us/step - loss: 6.3266 - acc: 0.2422 - val_loss: 6.3293 - val_acc: 0.2355\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/10\n",
      "319680/319680 [==============================] - 25s 79us/step - loss: 8.3579 - acc: 0.1100 - val_loss: 7.2853 - val_acc: 0.1978\n",
      "Epoch 2/10\n",
      "319680/319680 [==============================] - 26s 80us/step - loss: 7.0363 - acc: 0.2095 - val_loss: 6.8432 - val_acc: 0.1778\n",
      "Epoch 3/10\n",
      "319680/319680 [==============================] - 26s 80us/step - loss: 6.7254 - acc: 0.2268 - val_loss: 6.6296 - val_acc: 0.2321\n",
      "Epoch 4/10\n",
      "319680/319680 [==============================] - 26s 81us/step - loss: 6.5659 - acc: 0.2326 - val_loss: 6.5144 - val_acc: 0.2428\n",
      "Epoch 5/10\n",
      "319680/319680 [==============================] - 26s 83us/step - loss: 6.4677 - acc: 0.2374 - val_loss: 6.4360 - val_acc: 0.2275\n",
      "Epoch 6/10\n",
      "319680/319680 [==============================] - 26s 80us/step - loss: 6.4014 - acc: 0.2395 - val_loss: 6.3852 - val_acc: 0.2031\n",
      "Epoch 7/10\n",
      "319680/319680 [==============================] - 26s 80us/step - loss: 6.3517 - acc: 0.2406 - val_loss: 6.3385 - val_acc: 0.2453\n",
      "Epoch 8/10\n",
      "319680/319680 [==============================] - 26s 82us/step - loss: 6.3116 - acc: 0.2422 - val_loss: 6.3024 - val_acc: 0.1875\n",
      "Epoch 9/10\n",
      "319680/319680 [==============================] - 25s 78us/step - loss: 6.2769 - acc: 0.2433 - val_loss: 6.2674 - val_acc: 0.2437\n",
      "Epoch 10/10\n",
      "319680/319680 [==============================] - 26s 83us/step - loss: 6.2471 - acc: 0.2441 - val_loss: 6.2410 - val_acc: 0.2253\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/10\n",
      "319680/319680 [==============================] - 29s 90us/step - loss: 8.2108 - acc: 0.1185 - val_loss: 7.1685 - val_acc: 0.1801\n",
      "Epoch 2/10\n",
      "319680/319680 [==============================] - 30s 93us/step - loss: 6.9005 - acc: 0.2172 - val_loss: 6.6994 - val_acc: 0.2346\n",
      "Epoch 3/10\n",
      "319680/319680 [==============================] - 28s 88us/step - loss: 6.5877 - acc: 0.2316 - val_loss: 6.4931 - val_acc: 0.2514\n",
      "Epoch 4/10\n",
      "319680/319680 [==============================] - 27s 86us/step - loss: 6.4346 - acc: 0.2390 - val_loss: 6.3882 - val_acc: 0.2249\n",
      "Epoch 5/10\n",
      "319680/319680 [==============================] - 29s 90us/step - loss: 6.3418 - acc: 0.2422 - val_loss: 6.3129 - val_acc: 0.2934\n",
      "Epoch 6/10\n",
      "319680/319680 [==============================] - 28s 87us/step - loss: 6.2772 - acc: 0.2447 - val_loss: 6.2532 - val_acc: 0.2477\n",
      "Epoch 7/10\n",
      "319680/319680 [==============================] - 26s 82us/step - loss: 6.2263 - acc: 0.2482 - val_loss: 6.2128 - val_acc: 0.2166\n",
      "Epoch 8/10\n",
      "319680/319680 [==============================] - 27s 84us/step - loss: 6.1881 - acc: 0.2497 - val_loss: 6.1835 - val_acc: 0.2159\n",
      "Epoch 9/10\n",
      "319680/319680 [==============================] - 27s 83us/step - loss: 6.1576 - acc: 0.2496 - val_loss: 6.1579 - val_acc: 0.2728\n",
      "Epoch 10/10\n",
      "319680/319680 [==============================] - 28s 87us/step - loss: 6.1329 - acc: 0.2512 - val_loss: 6.1348 - val_acc: 0.2591\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/10\n",
      "319680/319680 [==============================] - 32s 99us/step - loss: 8.1597 - acc: 0.1266 - val_loss: 7.1175 - val_acc: 0.1924\n",
      "Epoch 2/10\n",
      "319680/319680 [==============================] - 29s 89us/step - loss: 6.8519 - acc: 0.2212 - val_loss: 6.6509 - val_acc: 0.2388\n",
      "Epoch 3/10\n",
      "319680/319680 [==============================] - 27s 85us/step - loss: 6.5351 - acc: 0.2351 - val_loss: 6.4489 - val_acc: 0.2186\n",
      "Epoch 4/10\n",
      "319680/319680 [==============================] - 28s 87us/step - loss: 6.3754 - acc: 0.2396 - val_loss: 6.3319 - val_acc: 0.2167\n",
      "Epoch 5/10\n",
      "319680/319680 [==============================] - 27s 85us/step - loss: 6.2799 - acc: 0.2439 - val_loss: 6.2653 - val_acc: 0.2045\n",
      "Epoch 6/10\n",
      "319680/319680 [==============================] - 28s 87us/step - loss: 6.2149 - acc: 0.2457 - val_loss: 6.2053 - val_acc: 0.2277\n",
      "Epoch 7/10\n",
      "319680/319680 [==============================] - 27s 86us/step - loss: 6.1657 - acc: 0.2480 - val_loss: 6.1485 - val_acc: 0.2148\n",
      "Epoch 8/10\n",
      "319680/319680 [==============================] - 28s 87us/step - loss: 6.1283 - acc: 0.2496 - val_loss: 6.1186 - val_acc: 0.2167\n",
      "Epoch 9/10\n",
      "319680/319680 [==============================] - 27s 85us/step - loss: 6.0970 - acc: 0.2500 - val_loss: 6.0998 - val_acc: 0.3014\n",
      "Epoch 10/10\n",
      "319680/319680 [==============================] - 28s 87us/step - loss: 6.0705 - acc: 0.2514 - val_loss: 6.0748 - val_acc: 0.2548\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/10\n",
      "319680/319680 [==============================] - 30s 95us/step - loss: 8.0962 - acc: 0.1244 - val_loss: 7.0693 - val_acc: 0.1592\n",
      "Epoch 2/10\n",
      "319680/319680 [==============================] - 29s 89us/step - loss: 6.7953 - acc: 0.2195 - val_loss: 6.5983 - val_acc: 0.2353\n",
      "Epoch 3/10\n",
      "319680/319680 [==============================] - 29s 92us/step - loss: 6.4819 - acc: 0.2333 - val_loss: 6.3971 - val_acc: 0.2375\n",
      "Epoch 4/10\n",
      "319680/319680 [==============================] - 31s 96us/step - loss: 6.3315 - acc: 0.2395 - val_loss: 6.2825 - val_acc: 0.2465\n",
      "Epoch 5/10\n",
      "319680/319680 [==============================] - 33s 105us/step - loss: 6.2431 - acc: 0.2417 - val_loss: 6.2230 - val_acc: 0.2252\n",
      "Epoch 6/10\n",
      "319680/319680 [==============================] - 31s 96us/step - loss: 6.1849 - acc: 0.2459 - val_loss: 6.1647 - val_acc: 0.2440\n",
      "Epoch 7/10\n",
      "319680/319680 [==============================] - 30s 93us/step - loss: 6.1409 - acc: 0.2464 - val_loss: 6.1340 - val_acc: 0.2863\n",
      "Epoch 8/10\n",
      "319680/319680 [==============================] - 29s 91us/step - loss: 6.1072 - acc: 0.2481 - val_loss: 6.1008 - val_acc: 0.2534\n",
      "Epoch 9/10\n",
      "319680/319680 [==============================] - 30s 94us/step - loss: 6.0805 - acc: 0.2497 - val_loss: 6.0814 - val_acc: 0.2939\n",
      "Epoch 10/10\n",
      "319680/319680 [==============================] - 29s 92us/step - loss: 6.0590 - acc: 0.2511 - val_loss: 6.0568 - val_acc: 0.3006\n"
     ]
    }
   ],
   "source": [
    "acc_list = []\n",
    "for n_neurons in range(50, 501, 50):\n",
    "    input1 = Input(shape=(84,))\n",
    "    w1 = Dense(n_neurons, activation='relu', name='weight1')\n",
    "    dense1 = w1(input1)\n",
    "    w2 = Dense(48, activation='softmax', name='weight2')\n",
    "    output1 = w2(dense1)\n",
    "\n",
    "    model = Model(inputs=[input1], outputs=[output1])\n",
    "    model.compile(optimizer='adam', loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "    h = model.fit(x=[ctrain_x], y=[ctrain_y], validation_data=[cvalid_x, cvalid_y], epochs=10)\n",
    "    \n",
    "    # 預測測試集\n",
    "    predicted = model.predict(ctest_x).reshape(ctest_x.shape[0], 4, 12)\n",
    "    \n",
    "    # 將預測結果(one-hot編碼)轉回一般數值\n",
    "    labels = []\n",
    "    ans = []\n",
    "    for i in range(0, len(test_y)):\n",
    "        labels.append(ctable.decode(test_y[i]))\n",
    "        ans.append(ctable.decode(predicted[i]))\n",
    "    # 計算測試資料正確率\n",
    "    acc = accuracy_score(ans, labels)\n",
    "    acc_list.append(acc)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下圖可發現，隨著神經元個數變多，accuracy持續成長，並逐漸緩慢下來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#coding:utf-8\n",
    "plt.rcParams['font.sans-serif']=['SimHei']   # 設定中文字體\n",
    "plt.title(u'不同神經元數對accuracy之影響(固定訓練10個epoch)')\n",
    "plt.xlabel('number of neurons')\n",
    "plt.ylabel('test accuracy')\n",
    "plt.plot(np.arange(50, 501, 50), acc_list, label='test_accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 固定神經元個數為300，比較訓練不同 epoch 次數之影響，並畫出訓練過程loss與accuracy之變化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 15s 47us/step - loss: 8.4824 - val_loss: 7.3729\n",
      "accuracy:  0.5236861861861862\n",
      "Epoch 2/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 15s 46us/step - loss: 7.1383 - val_loss: 6.9652\n",
      "accuracy:  0.6608983983983984\n",
      "Epoch 3/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 15s 47us/step - loss: 6.8351 - val_loss: 6.7362\n",
      "accuracy:  0.7203703703703703\n",
      "Epoch 4/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 14s 44us/step - loss: 6.6663 - val_loss: 6.6123\n",
      "accuracy:  0.7586461461461461\n",
      "Epoch 5/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 13s 42us/step - loss: 6.5595 - val_loss: 6.5184\n",
      "accuracy:  0.7899649649649649\n",
      "Epoch 6/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 14s 45us/step - loss: 6.4859 - val_loss: 6.4618\n",
      "accuracy:  0.8142892892892893\n",
      "Epoch 7/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 15s 47us/step - loss: 6.4302 - val_loss: 6.4177\n",
      "accuracy:  0.8332207207207207\n",
      "Epoch 8/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 14s 44us/step - loss: 6.3823 - val_loss: 6.3727\n",
      "accuracy:  0.8457082082082082\n",
      "Epoch 9/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 66us/step - loss: 6.3422 - val_loss: 6.3362\n",
      "accuracy:  0.8575575575575576\n",
      "Epoch 10/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 17s 52us/step - loss: 6.3118 - val_loss: 6.3022\n",
      "accuracy:  0.8697822822822823\n",
      "Epoch 11/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 67us/step - loss: 6.2865 - val_loss: 6.2863\n",
      "accuracy:  0.8763263263263263\n",
      "Epoch 12/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 60us/step - loss: 6.2647 - val_loss: 6.2735\n",
      "accuracy:  0.8820445445445445\n",
      "Epoch 13/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 67us/step - loss: 6.2454 - val_loss: 6.2463\n",
      "accuracy:  0.8892392392392392\n",
      "Epoch 14/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 55us/step - loss: 6.2291 - val_loss: 6.2270\n",
      "accuracy:  0.8973598598598599\n",
      "Epoch 15/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 57us/step - loss: 6.2152 - val_loss: 6.2243\n",
      "accuracy:  0.8996496496496497\n",
      "Epoch 16/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 57us/step - loss: 6.2019 - val_loss: 6.2156\n",
      "accuracy:  0.899537037037037\n",
      "Epoch 17/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 17s 54us/step - loss: 6.1905 - val_loss: 6.2090\n",
      "accuracy:  0.9021771771771772\n",
      "Epoch 18/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 57us/step - loss: 6.1794 - val_loss: 6.1825\n",
      "accuracy:  0.9074824824824825\n",
      "Epoch 19/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 17s 53us/step - loss: 6.1693 - val_loss: 6.1744\n",
      "accuracy:  0.9116116116116116\n",
      "Epoch 20/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 57us/step - loss: 6.1593 - val_loss: 6.1737\n",
      "accuracy:  0.9110485485485486\n",
      "Epoch 21/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 55us/step - loss: 6.1505 - val_loss: 6.1632\n",
      "accuracy:  0.914039039039039\n",
      "Epoch 22/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 17s 54us/step - loss: 6.1426 - val_loss: 6.1466\n",
      "accuracy:  0.9157032032032032\n",
      "Epoch 23/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 58us/step - loss: 6.1360 - val_loss: 6.1338\n",
      "accuracy:  0.9212462462462463\n",
      "Epoch 24/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 17s 53us/step - loss: 6.1293 - val_loss: 6.1378\n",
      "accuracy:  0.9200575575575576\n",
      "Epoch 25/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 58us/step - loss: 6.1228 - val_loss: 6.1410\n",
      "accuracy:  0.9201701701701702\n",
      "Epoch 26/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 17s 54us/step - loss: 6.1169 - val_loss: 6.1122\n",
      "accuracy:  0.924049049049049\n",
      "Epoch 27/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 56us/step - loss: 6.1118 - val_loss: 6.1182\n",
      "accuracy:  0.9220345345345345\n",
      "Epoch 28/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 57us/step - loss: 6.1072 - val_loss: 6.1184\n",
      "accuracy:  0.9243993993993994\n",
      "Epoch 29/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 17s 54us/step - loss: 6.1024 - val_loss: 6.1205\n",
      "accuracy:  0.924411911911912\n",
      "Epoch 30/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 58us/step - loss: 6.0978 - val_loss: 6.0959\n",
      "accuracy:  0.9286661661661662\n",
      "Epoch 31/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 17s 54us/step - loss: 6.0935 - val_loss: 6.0975\n",
      "accuracy:  0.9291666666666667\n",
      "Epoch 32/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 57us/step - loss: 6.0900 - val_loss: 6.0947\n",
      "accuracy:  0.9312562562562563\n",
      "Epoch 33/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 58us/step - loss: 6.0860 - val_loss: 6.0901\n",
      "accuracy:  0.93008008008008\n",
      "Epoch 34/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 56us/step - loss: 6.0819 - val_loss: 6.0823\n",
      "accuracy:  0.9316441441441441\n",
      "Epoch 35/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 61us/step - loss: 6.0781 - val_loss: 6.0826\n",
      "accuracy:  0.932982982982983\n",
      "Epoch 36/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 55us/step - loss: 6.0736 - val_loss: 6.0782\n",
      "accuracy:  0.9326326326326326\n",
      "Epoch 37/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 59us/step - loss: 6.0697 - val_loss: 6.0836\n",
      "accuracy:  0.9338463463463463\n",
      "Epoch 38/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 55us/step - loss: 6.0667 - val_loss: 6.0828\n",
      "accuracy:  0.9316316316316317\n",
      "Epoch 39/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 57us/step - loss: 6.0637 - val_loss: 6.0706\n",
      "accuracy:  0.9367242242242242\n",
      "Epoch 40/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 58us/step - loss: 6.0600 - val_loss: 6.0834\n",
      "accuracy:  0.9343093093093093\n",
      "Epoch 41/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 57us/step - loss: 6.0577 - val_loss: 6.0645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9355855855855856\n",
      "Epoch 42/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 59us/step - loss: 6.0540 - val_loss: 6.0655\n",
      "accuracy:  0.9367242242242242\n",
      "Epoch 43/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 17s 55us/step - loss: 6.0510 - val_loss: 6.0584\n",
      "accuracy:  0.9401151151151151\n",
      "Epoch 44/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 58us/step - loss: 6.0477 - val_loss: 6.0639\n",
      "accuracy:  0.9397022022022022\n",
      "Epoch 45/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 56us/step - loss: 6.0450 - val_loss: 6.0489\n",
      "accuracy:  0.9407157157157157\n",
      "Epoch 46/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 56us/step - loss: 6.0420 - val_loss: 6.0459\n",
      "accuracy:  0.9417667667667667\n",
      "Epoch 47/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 59us/step - loss: 6.0392 - val_loss: 6.0474\n",
      "accuracy:  0.9422797797797797\n",
      "Epoch 48/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 17s 54us/step - loss: 6.0367 - val_loss: 6.0585\n",
      "accuracy:  0.9396146146146146\n",
      "Epoch 49/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 58us/step - loss: 6.0342 - val_loss: 6.0555\n",
      "accuracy:  0.9403153153153153\n",
      "Epoch 50/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 57us/step - loss: 6.0320 - val_loss: 6.0462\n",
      "accuracy:  0.9420920920920921\n",
      "Epoch 51/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 63us/step - loss: 6.0298 - val_loss: 6.0418\n",
      "accuracy:  0.9426676676676676\n",
      "Epoch 52/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 61us/step - loss: 6.0276 - val_loss: 6.0424\n",
      "accuracy:  0.9431556556556556\n",
      "Epoch 53/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 22s 68us/step - loss: 6.0255 - val_loss: 6.0342\n",
      "accuracy:  0.9437937937937938\n",
      "Epoch 54/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 62us/step - loss: 6.0241 - val_loss: 6.0381\n",
      "accuracy:  0.9436936936936937\n",
      "Epoch 55/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 57us/step - loss: 6.0225 - val_loss: 6.0348\n",
      "accuracy:  0.9445445445445445\n",
      "Epoch 56/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 58us/step - loss: 6.0208 - val_loss: 6.0270\n",
      "accuracy:  0.9453203203203203\n",
      "Epoch 57/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 57us/step - loss: 6.0189 - val_loss: 6.0274\n",
      "accuracy:  0.9441316316316316\n",
      "Epoch 58/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 56us/step - loss: 6.0175 - val_loss: 6.0279\n",
      "accuracy:  0.9436061061061061\n",
      "Epoch 59/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 65us/step - loss: 6.0164 - val_loss: 6.0256\n",
      "accuracy:  0.9474099099099099\n",
      "Epoch 60/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 17s 54us/step - loss: 6.0149 - val_loss: 6.0329\n",
      "accuracy:  0.9429054054054054\n",
      "Epoch 61/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 58us/step - loss: 6.0130 - val_loss: 6.0226\n",
      "accuracy:  0.9463713713713714\n",
      "Epoch 62/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 55us/step - loss: 6.0122 - val_loss: 6.0208\n",
      "accuracy:  0.9480855855855855\n",
      "Epoch 63/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 58us/step - loss: 6.0109 - val_loss: 6.0268\n",
      "accuracy:  0.9475725725725725\n",
      "Epoch 64/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 59us/step - loss: 6.0094 - val_loss: 6.0182\n",
      "accuracy:  0.9464464464464465\n",
      "Epoch 65/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 17s 54us/step - loss: 6.0086 - val_loss: 6.0251\n",
      "accuracy:  0.9459084084084084\n",
      "Epoch 66/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 58us/step - loss: 6.0078 - val_loss: 6.0137\n",
      "accuracy:  0.9461961961961962\n",
      "Epoch 67/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 17s 55us/step - loss: 6.0059 - val_loss: 6.0066\n",
      "accuracy:  0.9482232232232233\n",
      "Epoch 68/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 58us/step - loss: 6.0056 - val_loss: 6.0146\n",
      "accuracy:  0.9457457457457458\n",
      "Epoch 69/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 57us/step - loss: 6.0045 - val_loss: 6.0133\n",
      "accuracy:  0.9472597597597597\n",
      "Epoch 70/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 55us/step - loss: 6.0034 - val_loss: 6.0084\n",
      "accuracy:  0.9493243243243243\n",
      "Epoch 71/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 59us/step - loss: 6.0022 - val_loss: 6.0150\n",
      "accuracy:  0.9484734734734734\n",
      "Epoch 72/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 17s 54us/step - loss: 6.0018 - val_loss: 6.0125\n",
      "accuracy:  0.9488988988988989\n",
      "Epoch 73/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 59us/step - loss: 6.0003 - val_loss: 6.0064\n",
      "accuracy:  0.9491366366366366\n",
      "Epoch 74/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 56us/step - loss: 5.9993 - val_loss: 6.0218\n",
      "accuracy:  0.9478478478478478\n",
      "Epoch 75/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 57us/step - loss: 5.9983 - val_loss: 6.0065\n",
      "accuracy:  0.9515265265265265\n",
      "Epoch 76/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 59us/step - loss: 5.9976 - val_loss: 6.0050\n",
      "accuracy:  0.9502127127127127\n",
      "Epoch 77/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 55us/step - loss: 5.9964 - val_loss: 6.0033\n",
      "accuracy:  0.9494244244244244\n",
      "Epoch 78/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 58us/step - loss: 5.9953 - val_loss: 6.0005\n",
      "accuracy:  0.950412912912913\n",
      "Epoch 79/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 17s 55us/step - loss: 5.9943 - val_loss: 6.0070\n",
      "accuracy:  0.9488988988988989\n",
      "Epoch 80/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 57us/step - loss: 5.9929 - val_loss: 6.0061\n",
      "accuracy:  0.9492867867867868\n",
      "Epoch 81/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 58us/step - loss: 5.9920 - val_loss: 6.0071\n",
      "accuracy:  0.9472222222222222\n",
      "Epoch 82/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319680/319680 [==============================] - 17s 55us/step - loss: 5.9905 - val_loss: 6.0046\n",
      "accuracy:  0.9497622622622622\n",
      "Epoch 83/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 58us/step - loss: 5.9902 - val_loss: 6.0051\n",
      "accuracy:  0.9499124124124124\n",
      "Epoch 84/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 17s 54us/step - loss: 5.9891 - val_loss: 6.0101\n",
      "accuracy:  0.9501626626626627\n",
      "Epoch 85/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 58us/step - loss: 5.9889 - val_loss: 6.0006\n",
      "accuracy:  0.9508258258258259\n",
      "Epoch 86/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 55us/step - loss: 5.9878 - val_loss: 5.9907\n",
      "accuracy:  0.9521896896896896\n",
      "Epoch 87/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 57us/step - loss: 5.9874 - val_loss: 5.9982\n",
      "accuracy:  0.9511511511511511\n",
      "Epoch 88/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 58us/step - loss: 5.9868 - val_loss: 5.9944\n",
      "accuracy:  0.9512387387387388\n",
      "Epoch 89/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 55us/step - loss: 5.9860 - val_loss: 6.0005\n",
      "accuracy:  0.9520145145145145\n",
      "Epoch 90/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 58us/step - loss: 5.9851 - val_loss: 5.9869\n",
      "accuracy:  0.952027027027027\n",
      "Epoch 91/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 17s 54us/step - loss: 5.9843 - val_loss: 6.0002\n",
      "accuracy:  0.9502252252252252\n",
      "Epoch 92/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 58us/step - loss: 5.9840 - val_loss: 5.9946\n",
      "accuracy:  0.9526776776776776\n",
      "Epoch 93/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 58us/step - loss: 5.9835 - val_loss: 5.9957\n",
      "accuracy:  0.9497747747747748\n",
      "Epoch 94/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 55us/step - loss: 5.9822 - val_loss: 5.9996\n",
      "accuracy:  0.9524024024024024\n",
      "Epoch 95/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 59us/step - loss: 5.9819 - val_loss: 6.0012\n",
      "accuracy:  0.9516016016016016\n",
      "Epoch 96/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 17s 53us/step - loss: 5.9815 - val_loss: 5.9863\n",
      "accuracy:  0.9536911911911912\n",
      "Epoch 97/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 57us/step - loss: 5.9810 - val_loss: 6.0118\n",
      "accuracy:  0.9501501501501501\n",
      "Epoch 98/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 17s 54us/step - loss: 5.9807 - val_loss: 5.9905\n",
      "accuracy:  0.9517017017017017\n",
      "Epoch 99/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 56us/step - loss: 5.9799 - val_loss: 5.9929\n",
      "accuracy:  0.9531781781781782\n",
      "Epoch 100/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 57us/step - loss: 5.9793 - val_loss: 5.9947\n",
      "accuracy:  0.951026026026026\n",
      "Epoch 101/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 17s 54us/step - loss: 5.9783 - val_loss: 5.9883\n",
      "accuracy:  0.9522647647647647\n",
      "Epoch 102/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 59us/step - loss: 5.9775 - val_loss: 5.9967\n",
      "accuracy:  0.9521521521521521\n",
      "Epoch 103/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 17s 53us/step - loss: 5.9774 - val_loss: 5.9864\n",
      "accuracy:  0.9523023023023023\n",
      "Epoch 104/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 56us/step - loss: 5.9768 - val_loss: 5.9927\n",
      "accuracy:  0.9556556556556557\n",
      "Epoch 105/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 56us/step - loss: 5.9764 - val_loss: 5.9802\n",
      "accuracy:  0.9535535535535535\n",
      "Epoch 106/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 17s 54us/step - loss: 5.9756 - val_loss: 5.9862\n",
      "accuracy:  0.9540165165165165\n",
      "Epoch 107/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 57us/step - loss: 5.9749 - val_loss: 5.9920\n",
      "accuracy:  0.9528028028028028\n",
      "Epoch 108/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 17s 53us/step - loss: 5.9734 - val_loss: 5.9761\n",
      "accuracy:  0.9533783783783784\n",
      "Epoch 109/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 57us/step - loss: 5.9728 - val_loss: 5.9789\n",
      "accuracy:  0.9533283283283284\n",
      "Epoch 110/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 17s 54us/step - loss: 5.9720 - val_loss: 5.9795\n",
      "accuracy:  0.9552677677677678\n",
      "Epoch 111/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 56us/step - loss: 5.9716 - val_loss: 5.9750\n",
      "accuracy:  0.9526276276276276\n",
      "Epoch 112/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 66us/step - loss: 5.9708 - val_loss: 5.9881\n",
      "accuracy:  0.9540540540540541\n",
      "Epoch 113/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 57us/step - loss: 5.9702 - val_loss: 5.9821\n",
      "accuracy:  0.9551176176176176\n",
      "Epoch 114/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 59us/step - loss: 5.9700 - val_loss: 5.9715\n",
      "accuracy:  0.9553803803803804\n",
      "Epoch 115/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 55us/step - loss: 5.9691 - val_loss: 5.9687\n",
      "accuracy:  0.9542417417417417\n",
      "Epoch 116/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 59us/step - loss: 5.9693 - val_loss: 5.9848\n",
      "accuracy:  0.9541041041041041\n",
      "Epoch 117/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 57us/step - loss: 5.9686 - val_loss: 5.9817\n",
      "accuracy:  0.9532657657657657\n",
      "Epoch 118/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 56us/step - loss: 5.9685 - val_loss: 5.9753\n",
      "accuracy:  0.9533783783783784\n",
      "Epoch 119/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 59us/step - loss: 5.9671 - val_loss: 5.9786\n",
      "accuracy:  0.9526401401401401\n",
      "Epoch 120/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 55us/step - loss: 5.9669 - val_loss: 5.9833\n",
      "accuracy:  0.9528403403403404\n",
      "Epoch 121/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 59us/step - loss: 5.9667 - val_loss: 5.9837\n",
      "accuracy:  0.9549049049049049\n",
      "Epoch 122/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 57us/step - loss: 5.9656 - val_loss: 5.9750\n",
      "accuracy:  0.9546921921921921\n",
      "Epoch 123/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319680/319680 [==============================] - 18s 57us/step - loss: 5.9655 - val_loss: 5.9922\n",
      "accuracy:  0.9536411411411412\n",
      "Epoch 124/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 59us/step - loss: 5.9651 - val_loss: 5.9852\n",
      "accuracy:  0.952977977977978\n",
      "Epoch 125/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 55us/step - loss: 5.9642 - val_loss: 5.9711\n",
      "accuracy:  0.9551426426426426\n",
      "Epoch 126/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 59us/step - loss: 5.9636 - val_loss: 5.9713\n",
      "accuracy:  0.955492992992993\n",
      "Epoch 127/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 55us/step - loss: 5.9631 - val_loss: 5.9789\n",
      "accuracy:  0.9556806806806807\n",
      "Epoch 128/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 59us/step - loss: 5.9624 - val_loss: 5.9670\n",
      "accuracy:  0.9558558558558559\n",
      "Epoch 129/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 57us/step - loss: 5.9627 - val_loss: 5.9796\n",
      "accuracy:  0.9556306306306306\n",
      "Epoch 130/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 57us/step - loss: 5.9616 - val_loss: 5.9715\n",
      "accuracy:  0.9558308308308309\n",
      "Epoch 131/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 66us/step - loss: 5.9617 - val_loss: 5.9795\n",
      "accuracy:  0.9544044044044044\n",
      "Epoch 132/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 59us/step - loss: 5.9612 - val_loss: 5.9877\n",
      "accuracy:  0.9522647647647647\n",
      "Epoch 133/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 57us/step - loss: 5.9605 - val_loss: 5.9720\n",
      "accuracy:  0.9553428428428429\n",
      "Epoch 134/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 18s 57us/step - loss: 5.9596 - val_loss: 5.9610\n",
      "accuracy:  0.9547797797797798\n",
      "Epoch 135/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 58us/step - loss: 5.9595 - val_loss: 5.9663\n",
      "accuracy:  0.9555805805805806\n",
      "Epoch 136/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 23s 71us/step - loss: 5.9588 - val_loss: 5.9683\n",
      "accuracy:  0.9563188188188189\n",
      "Epoch 137/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 63us/step - loss: 5.9586 - val_loss: 5.9735\n",
      "accuracy:  0.9550175175175175\n",
      "Epoch 138/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 66us/step - loss: 5.9576 - val_loss: 5.9732\n",
      "accuracy:  0.9563063063063063\n",
      "Epoch 139/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 64us/step - loss: 5.9579 - val_loss: 5.9637\n",
      "accuracy:  0.9556056056056056\n",
      "Epoch 140/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 65us/step - loss: 5.9572 - val_loss: 5.9655\n",
      "accuracy:  0.9573948948948949\n",
      "Epoch 141/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 65us/step - loss: 5.9566 - val_loss: 5.9846\n",
      "accuracy:  0.9534284284284285\n",
      "Epoch 142/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 61us/step - loss: 5.9564 - val_loss: 5.9712\n",
      "accuracy:  0.9556556556556557\n",
      "Epoch 143/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 67us/step - loss: 5.9556 - val_loss: 5.9613\n",
      "accuracy:  0.9562437437437438\n",
      "Epoch 144/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 63us/step - loss: 5.9550 - val_loss: 5.9727\n",
      "accuracy:  0.9572072072072072\n",
      "Epoch 145/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 66us/step - loss: 5.9551 - val_loss: 5.9595\n",
      "accuracy:  0.9575325325325326\n",
      "Epoch 146/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 64us/step - loss: 5.9539 - val_loss: 5.9634\n",
      "accuracy:  0.9567692692692693\n",
      "Epoch 147/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 63us/step - loss: 5.9531 - val_loss: 5.9624\n",
      "accuracy:  0.9570695695695696\n",
      "Epoch 148/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 22s 67us/step - loss: 5.9529 - val_loss: 5.9570\n",
      "accuracy:  0.9572447447447447\n",
      "Epoch 149/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 64us/step - loss: 5.9532 - val_loss: 5.9635\n",
      "accuracy:  0.9581581581581582\n",
      "Epoch 150/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 66us/step - loss: 5.9522 - val_loss: 5.9589\n",
      "accuracy:  0.9573073073073073\n",
      "Epoch 151/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 63us/step - loss: 5.9517 - val_loss: 5.9564\n",
      "accuracy:  0.9568943943943944\n",
      "Epoch 152/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 64us/step - loss: 5.9517 - val_loss: 5.9617\n",
      "accuracy:  0.9578578578578578\n",
      "Epoch 153/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 66us/step - loss: 5.9509 - val_loss: 5.9637\n",
      "accuracy:  0.9550675675675676\n",
      "Epoch 154/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 64us/step - loss: 5.9507 - val_loss: 5.9527\n",
      "accuracy:  0.9578328328328328\n",
      "Epoch 155/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 66us/step - loss: 5.9499 - val_loss: 5.9554\n",
      "accuracy:  0.9572947947947948\n",
      "Epoch 156/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 63us/step - loss: 5.9493 - val_loss: 5.9708\n",
      "accuracy:  0.95508008008008\n",
      "Epoch 157/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 65us/step - loss: 5.9489 - val_loss: 5.9626\n",
      "accuracy:  0.9570945945945946\n",
      "Epoch 158/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 65us/step - loss: 5.9488 - val_loss: 5.9564\n",
      "accuracy:  0.9570195195195195\n",
      "Epoch 159/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 63us/step - loss: 5.9481 - val_loss: 5.9708\n",
      "accuracy:  0.9582332332332333\n",
      "Epoch 160/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 65us/step - loss: 5.9480 - val_loss: 5.9571\n",
      "accuracy:  0.9571571571571572\n",
      "Epoch 161/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 61us/step - loss: 5.9479 - val_loss: 5.9666\n",
      "accuracy:  0.9563188188188189\n",
      "Epoch 162/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 65us/step - loss: 5.9468 - val_loss: 5.9569\n",
      "accuracy:  0.9580205205205206\n",
      "Epoch 163/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 63us/step - loss: 5.9466 - val_loss: 5.9473\n",
      "accuracy:  0.9582707707707707\n",
      "Epoch 164/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319680/319680 [==============================] - 20s 62us/step - loss: 5.9467 - val_loss: 5.9645\n",
      "accuracy:  0.9575700700700701\n",
      "Epoch 165/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 67us/step - loss: 5.9464 - val_loss: 5.9557\n",
      "accuracy:  0.9566566566566567\n",
      "Epoch 166/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 19s 61us/step - loss: 5.9461 - val_loss: 5.9595\n",
      "accuracy:  0.9578328328328328\n",
      "Epoch 167/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 66us/step - loss: 5.9455 - val_loss: 5.9579\n",
      "accuracy:  0.9568193193193193\n",
      "Epoch 168/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 63us/step - loss: 5.9461 - val_loss: 5.9591\n",
      "accuracy:  0.958008008008008\n",
      "Epoch 169/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 63us/step - loss: 5.9455 - val_loss: 5.9531\n",
      "accuracy:  0.9573448448448448\n",
      "Epoch 170/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 22s 68us/step - loss: 5.9453 - val_loss: 5.9589\n",
      "accuracy:  0.9580455455455456\n",
      "Epoch 171/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 62us/step - loss: 5.9446 - val_loss: 5.9514\n",
      "accuracy:  0.9593843843843843\n",
      "Epoch 172/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 67us/step - loss: 5.9446 - val_loss: 5.9544\n",
      "accuracy:  0.9567192192192192\n",
      "Epoch 173/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 61us/step - loss: 5.9441 - val_loss: 5.9716\n",
      "accuracy:  0.9566566566566567\n",
      "Epoch 174/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 65us/step - loss: 5.9442 - val_loss: 5.9481\n",
      "accuracy:  0.9592967967967968\n",
      "Epoch 175/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 67us/step - loss: 5.9436 - val_loss: 5.9623\n",
      "accuracy:  0.9564439439439439\n",
      "Epoch 176/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 62us/step - loss: 5.9434 - val_loss: 5.9539\n",
      "accuracy:  0.9582332332332333\n",
      "Epoch 177/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 67us/step - loss: 5.9432 - val_loss: 5.9654\n",
      "accuracy:  0.9575075075075075\n",
      "Epoch 178/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 62us/step - loss: 5.9429 - val_loss: 5.9442\n",
      "accuracy:  0.958521021021021\n",
      "Epoch 179/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 66us/step - loss: 5.9429 - val_loss: 5.9585\n",
      "accuracy:  0.9563438438438439\n",
      "Epoch 180/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 65us/step - loss: 5.9430 - val_loss: 5.9612\n",
      "accuracy:  0.9584209209209209\n",
      "Epoch 181/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 63us/step - loss: 5.9425 - val_loss: 5.9555\n",
      "accuracy:  0.9576701701701702\n",
      "Epoch 182/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 66us/step - loss: 5.9422 - val_loss: 5.9478\n",
      "accuracy:  0.9578828828828829\n",
      "Epoch 183/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 62us/step - loss: 5.9421 - val_loss: 5.9570\n",
      "accuracy:  0.9559059059059059\n",
      "Epoch 184/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 66us/step - loss: 5.9417 - val_loss: 5.9553\n",
      "accuracy:  0.9570945945945946\n",
      "Epoch 185/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 64us/step - loss: 5.9418 - val_loss: 5.9551\n",
      "accuracy:  0.9592842842842842\n",
      "Epoch 186/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 63us/step - loss: 5.9408 - val_loss: 5.9470\n",
      "accuracy:  0.9582707707707707\n",
      "Epoch 187/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 66us/step - loss: 5.9405 - val_loss: 5.9588\n",
      "accuracy:  0.9565065065065065\n",
      "Epoch 188/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 62us/step - loss: 5.9404 - val_loss: 5.9503\n",
      "accuracy:  0.9586586586586586\n",
      "Epoch 189/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 65us/step - loss: 5.9401 - val_loss: 5.9503\n",
      "accuracy:  0.9585335335335335\n",
      "Epoch 190/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 63us/step - loss: 5.9397 - val_loss: 5.9401\n",
      "accuracy:  0.9576576576576576\n",
      "Epoch 191/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 64us/step - loss: 5.9400 - val_loss: 5.9564\n",
      "accuracy:  0.9563688688688688\n",
      "Epoch 192/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 65us/step - loss: 5.9389 - val_loss: 5.9533\n",
      "accuracy:  0.9565315315315316\n",
      "Epoch 193/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 62us/step - loss: 5.9397 - val_loss: 5.9605\n",
      "accuracy:  0.9582207207207207\n",
      "Epoch 194/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 66us/step - loss: 5.9390 - val_loss: 5.9584\n",
      "accuracy:  0.9595595595595595\n",
      "Epoch 195/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 63us/step - loss: 5.9386 - val_loss: 5.9488\n",
      "accuracy:  0.960447947947948\n",
      "Epoch 196/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 22s 68us/step - loss: 5.9381 - val_loss: 5.9515\n",
      "accuracy:  0.9574074074074074\n",
      "Epoch 197/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 64us/step - loss: 5.9376 - val_loss: 5.9469\n",
      "accuracy:  0.95746996996997\n",
      "Epoch 198/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 63us/step - loss: 5.9378 - val_loss: 5.9457\n",
      "accuracy:  0.9603228228228228\n",
      "Epoch 199/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 21s 67us/step - loss: 5.9377 - val_loss: 5.9419\n",
      "accuracy:  0.959934934934935\n",
      "Epoch 200/200\n",
      "Train on 319680 samples, validate on 79920 samples\n",
      "Epoch 1/1\n",
      "319680/319680 [==============================] - 20s 62us/step - loss: 5.9372 - val_loss: 5.9575\n",
      "accuracy:  0.9580580580580581\n"
     ]
    }
   ],
   "source": [
    "epoch_size = 200\n",
    "input1 = Input(shape=(84,))\n",
    "w1 = Dense(300, activation='relu', name='weight1')\n",
    "dense1 = w1(input1)\n",
    "w2 = Dense(48, activation='softmax', name='weight2')\n",
    "output1 = w2(dense1)\n",
    "\n",
    "model = Model(inputs=[input1], outputs=[output1])\n",
    "model.compile(optimizer='adam', loss=['categorical_crossentropy'])\n",
    "\n",
    "accuracy_list = []\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "for i in range(1, epoch_size+1):\n",
    "    print('Epoch {}/{}'.format(i, epoch_size))\n",
    "    h = model.fit(x=[ctrain_x], y=[ctrain_y], validation_data=[cvalid_x, cvalid_y])\n",
    "    train_loss_list.append(h.history['loss'][0])\n",
    "    val_loss_list.append(h.history['val_loss'][0])\n",
    "    \n",
    "    # 預測測試集\n",
    "    predicted = model.predict(cvalid_x).reshape(cvalid_x.shape[0], 4, 12)\n",
    "    # 將預測結果(one-hot編碼)轉回一般數值\n",
    "    labels = []\n",
    "    ans = []\n",
    "    for i in range(0, len(valid_y)):\n",
    "      labels.append(ctable.decode(valid_y[i]))\n",
    "      ans.append(ctable.decode(predicted[i]))\n",
    "\n",
    "    # 計算正確率\n",
    "    acc = accuracy_score(ans, labels)\n",
    "    print('accuracy: ', acc)\n",
    "    accuracy_list.append(acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下方兩張圖:\n",
    "## 第一張是隨著training epoch變多，訓練誤差和驗證誤差均持續降低。\n",
    "## 第二張圖則是隨著training epoch變多，驗證accuracy持續上升至收斂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 隨著 epoch 次數不同，loss 之變化\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(range(1, epoch_size+1, 1), train_loss_list, label='train_loss')\n",
    "plt.plot(range(1, epoch_size+1, 1), val_loss_list, label='valid_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 隨著 epoch 次數不同，accuracy 之變化\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.plot(accuracy_list, label='valid_accuracy' )\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用測試集評估最終結果，accuracy 0.9597"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9597297297297297\n"
     ]
    }
   ],
   "source": [
    "# 預測測試集\n",
    "predicted = model.predict(ctest_x).reshape(ctest_x.shape[0], 4, 12)\n",
    "# 將預測結果(one-hot編碼)轉回一般數值\n",
    "labels = []\n",
    "ans = []\n",
    "for i in range(0, len(ctest_y)):\n",
    "  labels.append(ctable.decode(test_y[i]))\n",
    "  ans.append(ctable.decode(predicted[i]))\n",
    "\n",
    "# 計算正確率\n",
    "acc = accuracy_score(ans, labels)\n",
    "print('accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
